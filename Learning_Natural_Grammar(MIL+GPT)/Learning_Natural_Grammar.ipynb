{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66d4be47-9d9f-4af4-ace4-d15718665457",
   "metadata": {},
   "source": [
    "# Learning Simplified Natural Grammar (MIL+GPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b8a6f9-108f-4b44-90d9-96e9c8d5383a",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34045e0c-5545-40d1-83da-582028246bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_api import *\n",
    "import sys\n",
    "sys.path.insert(0, '../../../')\n",
    "from PyGol_Final import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab as plb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdd83a0-1fef-44e0-9d1b-aa4424a52279",
   "metadata": {},
   "source": [
    "### Background knowledge, examples generation - PyGol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c3891e-9150-4330-bd8a-f1dc9a58d643",
   "metadata": {},
   "outputs": [],
   "source": [
    "BK_file, pos_example, neg_example, senetence_list= generate_bk_for_natural_language(\"facts.pl\", \"ex.pl\", \"BK_file.pl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155113e2-853b-4904-8042-31798fa55524",
   "metadata": {},
   "source": [
    "### List to guide the number of runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54612ff9-598c-4a87-87a1-6eab2e1ee48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_1 = [1, 2,5,10,15,20]\n",
    "number_of_tests = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5c7688-fcad-4c2e-8379-1d5e6f265ee7",
   "metadata": {},
   "source": [
    "### List to save the metrics from all the runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55c139d-63f4-4f00-af42-3286f9252e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_accuracy_pygol = []\n",
    "final_accuracy_GPT = []\n",
    "final_accuracy_hybrid = []\n",
    "default_accuracy = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa83701-e494-49e6-91e8-fa1a66803075",
   "metadata": {},
   "source": [
    "### Learning Phase (PyGol and GPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d1561a-ce6f-4277-9ae6-2b05bcb59f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in list_1:\n",
    "    print(\"Number of pos. examples:\", i)\n",
    "    acc_pygol =[]\n",
    "    acc_chat = []\n",
    "    acc_hybrid = []\n",
    "    for j in range(0,number_of_tests):\n",
    "        #positive train examples - PyGol\n",
    "        pos_train =  random.sample(pos_example, i)\n",
    "        #positive test examples - PyGol\n",
    "        pos_test = set(pos_example).difference(set(pos_train))\n",
    "        #negative train examples - PyGol\n",
    "        neg_train =  random.sample(neg_example, i)\n",
    "        #negative test examples - PyGol\n",
    "        neg_test = set(neg_example).difference(set(neg_train))\n",
    "        #positive train examples - GPT\n",
    "        pos_train_list = [senetence_list[i] for i in pos_train]\n",
    "        #positive test examples - GPT\n",
    "        pos_test_list = [senetence_list[i] for i in pos_test]\n",
    "        #negative train examples - GPT\n",
    "        neg_train_list = [senetence_list[i] for i in neg_train]\n",
    "        #negative test examples - GPT\n",
    "        neg_test_list = [senetence_list[i] for i in neg_test]\n",
    "        #Bottom clause generation - PyGol\n",
    "        P, N = bottom_clause_generation(constant_set = [], container = \"memory\", file = BK_file,\n",
    "                                        positive_example = pos_train, \n",
    "                                        negative_example = neg_train, \n",
    "                                        tqdm_disable=True)\n",
    "        #Hypothesis from PyGol\n",
    "        H = pygol_learn_natural_language(P,N)\n",
    "        #Proposition hypothesis to use in GPT\n",
    "        H_11 = hypo_prepositional(H)\n",
    "        # Test hypothesis from Pygol starts here\n",
    "        model = evaluate_theory_prolog(H, BK_file, pos_test, neg_test, verbose=True)\n",
    "        accuracy_pygol= model.accuracy\n",
    "        acc_pygol.append(accuracy_pygol)\n",
    "        # Test hypothesis from Pygol ends here\n",
    "        # Test hypothesis from GPT starts here\n",
    "        result_1 = evaluate_model_with_chatgpt(pos_train_list,neg_train_list, \n",
    "                                             pos_test_list, neg_test_list)\n",
    "        acc_GPT= calc_acc_gpt(result_1)\n",
    "        acc_chat.append(acc_GPT)\n",
    "        # Test hypothesis from GPT ends here\n",
    "        # Test hypothesis from GPT+MIL starts here\n",
    "        result_2 = evaluate_model_with_chatgpt_hybrid(pos_train_list,neg_train_list, \n",
    "                                             pos_test_list, neg_test_list,H_11)\n",
    "        acc_GPT_hybrid = calc_acc_gpt(result_2)\n",
    "        acc_hybrid.append(acc_GPT_hybrid)\n",
    "        # Test hypothesis from GPT_MIL ends here\n",
    "    print(\"\\tPyGol\",np.mean(acc_pygol))\n",
    "    final_accuracy_pygol.append(acc_pygol)\n",
    "    print(\"\\tGPT\",np.mean(acc_chat))\n",
    "    final_accuracy_GPT.append(acc_chat)\n",
    "    print(\"\\tGPT\",np.mean(acc_hybrid))\n",
    "    final_accuracy_hybrid.append(acc_hybrid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f93fa1d-d5a8-4b6d-aa56-e8e245d11914",
   "metadata": {},
   "source": [
    "### Set Default Accuracy list according to the length of items in 'list_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c4827e-22f1-4596-86e8-6a4e86cc3dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_list = [0.5, 0.5]\n",
    "for i in list_1:\n",
    "    default_accuracy.append(sample_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfb1b16-a88a-4170-8285-8b6bdcb7082d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plb.rcParams['font.size'] = 12\n",
    "\n",
    "means_default = []\n",
    "errors_default = []\n",
    "for sublist in default_accuracy:\n",
    "    np_sublist = np.array(sublist)\n",
    "    mean = np.mean(np_sublist)\n",
    "    std_dev = np.std(np_sublist)\n",
    "    sem = std_dev / np.sqrt(len(sublist))  \n",
    "    means_default.append(mean)\n",
    "    errors_default.append(sem)\n",
    "\n",
    "means_pygol = []\n",
    "errors_pygol = []\n",
    "for sublist in final_accuracy_pygol:\n",
    "    np_sublist = np.array(sublist)\n",
    "    mean = np.mean(np_sublist)\n",
    "    std_dev = np.std(np_sublist)\n",
    "    sem = std_dev / np.sqrt(len(sublist))  \n",
    "    means_pygol.append(mean)\n",
    "    errors_pygol.append(sem)\n",
    "\n",
    "means_chat = []\n",
    "errors_chat = []\n",
    "for sublist in final_accuracy_hybrid:\n",
    "    np_sublist = np.array(sublist)\n",
    "    mean = np.mean(np_sublist)\n",
    "    std_dev = np.std(np_sublist)\n",
    "    sem = std_dev / np.sqrt(len(sublist))  \n",
    "    means_chat.append(mean)\n",
    "    errors_chat.append(sem)\n",
    "\n",
    "means_chat_hybrid = []\n",
    "errors_chat_hybrid = []\n",
    "for sublist in final_accuracy_GPT:\n",
    "    np_sublist = np.array(sublist)\n",
    "    mean = np.mean(np_sublist)\n",
    "    std_dev = np.std(np_sublist)\n",
    "    sem = std_dev / np.sqrt(len(sublist))  \n",
    "    means_chat_hybrid.append(mean)\n",
    "    errors_chat_hybrid.append(sem)\n",
    "\n",
    "\n",
    "# Creating the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.errorbar(list_1, means_pygol, yerr=errors_pygol, fmt='--', capsize=5, ecolor='red', color='red', markersize=1, label='PyGol')\n",
    "plt.errorbar(list_1, means_chat, yerr=errors_chat, fmt='--', capsize=5, ecolor='blue', color='blue', markersize=1, label='GPT')\n",
    "plt.errorbar(list_1, means_chat_hybrid, yerr=errors_chat_hybrid, fmt='--', capsize=5, ecolor='green', color='green', markersize=1, label='GPT+PyGol')\n",
    "\n",
    "plt.errorbar(list_1, means_default, yerr=errors_default, fmt='--', capsize=5, ecolor='black', color='black', markersize=1, label='Deafult')\n",
    "\n",
    "custom_xtick_positions = [0, 4,  8,  12,  16, 20]\n",
    "custom_xtick_labels = ['0', '4',  '8',  '12',  '16',  '20']\n",
    "plt.xticks(ticks=custom_xtick_positions, labels=custom_xtick_labels)\n",
    "plt.legend()\n",
    "plt.xlabel('Number of examples')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlim(0.0, 20.1)  # Set x-axis range\n",
    "plt.ylim(0.4, 1.1)\n",
    "# Show the plot\n",
    "plt.legend( bbox_to_anchor=[1.1, 0.5], \n",
    "           loc='center', ncol=1)\n",
    "plt.savefig('plot.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c633686d-ac0a-47d6-a4bb-705bc4415659",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
